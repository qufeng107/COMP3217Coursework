{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "bf0f4eca-2474-450b-813e-49cb36348979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "0a81611a-cf67-4e22-a591-12f8470f85a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.881108</td>\n",
       "      <td>3.616019</td>\n",
       "      <td>3.601147</td>\n",
       "      <td>3.417457</td>\n",
       "      <td>2.723198</td>\n",
       "      <td>3.330895</td>\n",
       "      <td>3.723629</td>\n",
       "      <td>3.709737</td>\n",
       "      <td>5.848922</td>\n",
       "      <td>5.252491</td>\n",
       "      <td>...</td>\n",
       "      <td>6.215646</td>\n",
       "      <td>6.200070</td>\n",
       "      <td>5.842163</td>\n",
       "      <td>6.148899</td>\n",
       "      <td>5.333915</td>\n",
       "      <td>5.385360</td>\n",
       "      <td>4.851503</td>\n",
       "      <td>5.118068</td>\n",
       "      <td>5.113795</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.800883</td>\n",
       "      <td>3.210280</td>\n",
       "      <td>3.081142</td>\n",
       "      <td>3.498283</td>\n",
       "      <td>2.734601</td>\n",
       "      <td>3.105662</td>\n",
       "      <td>3.801500</td>\n",
       "      <td>3.796322</td>\n",
       "      <td>5.062057</td>\n",
       "      <td>4.987285</td>\n",
       "      <td>...</td>\n",
       "      <td>6.573781</td>\n",
       "      <td>5.749618</td>\n",
       "      <td>6.747027</td>\n",
       "      <td>6.537529</td>\n",
       "      <td>5.186155</td>\n",
       "      <td>5.420567</td>\n",
       "      <td>4.777363</td>\n",
       "      <td>5.120788</td>\n",
       "      <td>5.136329</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.964203</td>\n",
       "      <td>3.526702</td>\n",
       "      <td>3.255776</td>\n",
       "      <td>3.002896</td>\n",
       "      <td>2.812806</td>\n",
       "      <td>3.133526</td>\n",
       "      <td>3.635403</td>\n",
       "      <td>3.581440</td>\n",
       "      <td>5.607975</td>\n",
       "      <td>4.847624</td>\n",
       "      <td>...</td>\n",
       "      <td>6.162477</td>\n",
       "      <td>5.844224</td>\n",
       "      <td>6.073829</td>\n",
       "      <td>6.075009</td>\n",
       "      <td>5.572521</td>\n",
       "      <td>5.194047</td>\n",
       "      <td>4.771206</td>\n",
       "      <td>5.424286</td>\n",
       "      <td>5.628824</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.907543</td>\n",
       "      <td>3.201577</td>\n",
       "      <td>3.527606</td>\n",
       "      <td>2.861318</td>\n",
       "      <td>2.670537</td>\n",
       "      <td>3.311229</td>\n",
       "      <td>3.855523</td>\n",
       "      <td>3.877643</td>\n",
       "      <td>5.054537</td>\n",
       "      <td>4.982864</td>\n",
       "      <td>...</td>\n",
       "      <td>5.755873</td>\n",
       "      <td>5.531023</td>\n",
       "      <td>5.888425</td>\n",
       "      <td>6.627419</td>\n",
       "      <td>6.000076</td>\n",
       "      <td>5.059838</td>\n",
       "      <td>4.898310</td>\n",
       "      <td>5.887313</td>\n",
       "      <td>5.005183</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.051307</td>\n",
       "      <td>3.730421</td>\n",
       "      <td>3.192650</td>\n",
       "      <td>3.007943</td>\n",
       "      <td>2.954206</td>\n",
       "      <td>3.994215</td>\n",
       "      <td>3.670745</td>\n",
       "      <td>3.922720</td>\n",
       "      <td>5.561307</td>\n",
       "      <td>5.343242</td>\n",
       "      <td>...</td>\n",
       "      <td>5.712020</td>\n",
       "      <td>5.931136</td>\n",
       "      <td>6.635594</td>\n",
       "      <td>6.164542</td>\n",
       "      <td>5.439445</td>\n",
       "      <td>5.488501</td>\n",
       "      <td>4.781580</td>\n",
       "      <td>5.307637</td>\n",
       "      <td>5.480369</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>4.441229</td>\n",
       "      <td>4.115273</td>\n",
       "      <td>3.472402</td>\n",
       "      <td>2.745761</td>\n",
       "      <td>3.660351</td>\n",
       "      <td>4.093115</td>\n",
       "      <td>4.007465</td>\n",
       "      <td>4.291413</td>\n",
       "      <td>5.070075</td>\n",
       "      <td>5.110675</td>\n",
       "      <td>...</td>\n",
       "      <td>5.678653</td>\n",
       "      <td>5.965473</td>\n",
       "      <td>6.742951</td>\n",
       "      <td>6.404496</td>\n",
       "      <td>5.362002</td>\n",
       "      <td>5.981578</td>\n",
       "      <td>5.556540</td>\n",
       "      <td>5.991490</td>\n",
       "      <td>5.850924</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>4.016708</td>\n",
       "      <td>3.746175</td>\n",
       "      <td>3.199771</td>\n",
       "      <td>3.178246</td>\n",
       "      <td>3.459837</td>\n",
       "      <td>3.968765</td>\n",
       "      <td>4.398305</td>\n",
       "      <td>4.565947</td>\n",
       "      <td>5.133747</td>\n",
       "      <td>5.116910</td>\n",
       "      <td>...</td>\n",
       "      <td>6.506161</td>\n",
       "      <td>5.920921</td>\n",
       "      <td>6.675199</td>\n",
       "      <td>6.688912</td>\n",
       "      <td>5.470335</td>\n",
       "      <td>6.004166</td>\n",
       "      <td>5.682725</td>\n",
       "      <td>5.648218</td>\n",
       "      <td>5.464242</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>3.937626</td>\n",
       "      <td>3.803369</td>\n",
       "      <td>3.818526</td>\n",
       "      <td>3.293163</td>\n",
       "      <td>3.354351</td>\n",
       "      <td>3.720848</td>\n",
       "      <td>3.730639</td>\n",
       "      <td>4.349628</td>\n",
       "      <td>5.163885</td>\n",
       "      <td>4.562440</td>\n",
       "      <td>...</td>\n",
       "      <td>6.393017</td>\n",
       "      <td>5.727222</td>\n",
       "      <td>6.121406</td>\n",
       "      <td>6.107012</td>\n",
       "      <td>5.998606</td>\n",
       "      <td>6.053508</td>\n",
       "      <td>5.697338</td>\n",
       "      <td>5.854592</td>\n",
       "      <td>5.457969</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>4.252633</td>\n",
       "      <td>3.560719</td>\n",
       "      <td>3.353298</td>\n",
       "      <td>3.695314</td>\n",
       "      <td>3.451712</td>\n",
       "      <td>3.377892</td>\n",
       "      <td>4.307202</td>\n",
       "      <td>4.082620</td>\n",
       "      <td>5.769166</td>\n",
       "      <td>5.403883</td>\n",
       "      <td>...</td>\n",
       "      <td>6.438657</td>\n",
       "      <td>6.272212</td>\n",
       "      <td>6.747316</td>\n",
       "      <td>6.907396</td>\n",
       "      <td>5.848778</td>\n",
       "      <td>5.813068</td>\n",
       "      <td>5.720781</td>\n",
       "      <td>5.539517</td>\n",
       "      <td>5.741377</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>4.425644</td>\n",
       "      <td>4.136055</td>\n",
       "      <td>3.789301</td>\n",
       "      <td>3.395030</td>\n",
       "      <td>3.550975</td>\n",
       "      <td>4.012016</td>\n",
       "      <td>4.102223</td>\n",
       "      <td>4.446715</td>\n",
       "      <td>5.587647</td>\n",
       "      <td>5.135701</td>\n",
       "      <td>...</td>\n",
       "      <td>5.875079</td>\n",
       "      <td>6.124633</td>\n",
       "      <td>6.212417</td>\n",
       "      <td>6.455535</td>\n",
       "      <td>5.900350</td>\n",
       "      <td>5.848514</td>\n",
       "      <td>5.736068</td>\n",
       "      <td>5.372799</td>\n",
       "      <td>5.439422</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     3.881108  3.616019  3.601147  3.417457  2.723198  3.330895  3.723629   \n",
       "1     3.800883  3.210280  3.081142  3.498283  2.734601  3.105662  3.801500   \n",
       "2     3.964203  3.526702  3.255776  3.002896  2.812806  3.133526  3.635403   \n",
       "3     3.907543  3.201577  3.527606  2.861318  2.670537  3.311229  3.855523   \n",
       "4     4.051307  3.730421  3.192650  3.007943  2.954206  3.994215  3.670745   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995  4.441229  4.115273  3.472402  2.745761  3.660351  4.093115  4.007465   \n",
       "9996  4.016708  3.746175  3.199771  3.178246  3.459837  3.968765  4.398305   \n",
       "9997  3.937626  3.803369  3.818526  3.293163  3.354351  3.720848  3.730639   \n",
       "9998  4.252633  3.560719  3.353298  3.695314  3.451712  3.377892  4.307202   \n",
       "9999  4.425644  4.136055  3.789301  3.395030  3.550975  4.012016  4.102223   \n",
       "\n",
       "             7         8         9  ...        15        16        17  \\\n",
       "0     3.709737  5.848922  5.252491  ...  6.215646  6.200070  5.842163   \n",
       "1     3.796322  5.062057  4.987285  ...  6.573781  5.749618  6.747027   \n",
       "2     3.581440  5.607975  4.847624  ...  6.162477  5.844224  6.073829   \n",
       "3     3.877643  5.054537  4.982864  ...  5.755873  5.531023  5.888425   \n",
       "4     3.922720  5.561307  5.343242  ...  5.712020  5.931136  6.635594   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9995  4.291413  5.070075  5.110675  ...  5.678653  5.965473  6.742951   \n",
       "9996  4.565947  5.133747  5.116910  ...  6.506161  5.920921  6.675199   \n",
       "9997  4.349628  5.163885  4.562440  ...  6.393017  5.727222  6.121406   \n",
       "9998  4.082620  5.769166  5.403883  ...  6.438657  6.272212  6.747316   \n",
       "9999  4.446715  5.587647  5.135701  ...  5.875079  6.124633  6.212417   \n",
       "\n",
       "            18        19        20        21        22        23  label  \n",
       "0     6.148899  5.333915  5.385360  4.851503  5.118068  5.113795   True  \n",
       "1     6.537529  5.186155  5.420567  4.777363  5.120788  5.136329   True  \n",
       "2     6.075009  5.572521  5.194047  4.771206  5.424286  5.628824   True  \n",
       "3     6.627419  6.000076  5.059838  4.898310  5.887313  5.005183   True  \n",
       "4     6.164542  5.439445  5.488501  4.781580  5.307637  5.480369   True  \n",
       "...        ...       ...       ...       ...       ...       ...    ...  \n",
       "9995  6.404496  5.362002  5.981578  5.556540  5.991490  5.850924  False  \n",
       "9996  6.688912  5.470335  6.004166  5.682725  5.648218  5.464242  False  \n",
       "9997  6.107012  5.998606  6.053508  5.697338  5.854592  5.457969  False  \n",
       "9998  6.907396  5.848778  5.813068  5.720781  5.539517  5.741377  False  \n",
       "9999  6.455535  5.900350  5.848514  5.736068  5.372799  5.439422  False  \n",
       "\n",
       "[10000 rows x 25 columns]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read training data sets,\n",
    "\n",
    "hours = list(map(str, range(24))) + ['label']\n",
    "prices = pd.read_csv('./TrainingData.csv', names = hours,  converters = {'label': lambda x: not bool(int(x))})\n",
    "\n",
    "\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "4d2e3bea-43aa-4321-b37a-c8acbb7a597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 3.61601852939846\n",
      "True 3.21028027471593\n",
      "True 3.52670184047575\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "27e4378a-32ae-4b2c-a18a-f7936fe09bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = prices.sample(frac=0.8, random_state=10)\n",
    "train_y = train_x.pop('label')\n",
    "\n",
    "\n",
    "test_x = prices.drop(train_x.index)\n",
    "test_y = test_x.pop('label')\n",
    "\n",
    "# scale based on the training set\n",
    "scaler = MinMaxScaler()\n",
    "scaled_train = scaler.fit_transform(train_x)\n",
    "scaled_test = scaler.fit_transform(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "ed6c550e-3762-4dfe-b94a-be6506cb61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "b0243ca3-8fcc-4cb2-98e5-8dc79fc5f501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_41 (Dense)            (None, 1)                 25        \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(tf.keras.layers.Dense(1, input_shape=(24,)))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "3f4070f1-4f71-4294-9eac-030f156de67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "3058936d-75a8-48ce-8031-d45d56c471e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "250/250 [==============================] - 1s 882us/step - loss: 0.7222 - accuracy: 0.5889 - auc: 0.6357\n",
      "Epoch 2/150\n",
      "250/250 [==============================] - 0s 810us/step - loss: 0.6246 - accuracy: 0.6494 - auc: 0.7172\n",
      "Epoch 3/150\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.5965 - accuracy: 0.6874 - auc: 0.7654\n",
      "Epoch 4/150\n",
      "250/250 [==============================] - 0s 740us/step - loss: 0.5671 - accuracy: 0.7224 - auc: 0.8064\n",
      "Epoch 5/150\n",
      "250/250 [==============================] - 0s 720us/step - loss: 0.5378 - accuracy: 0.7499 - auc: 0.8369\n",
      "Epoch 6/150\n",
      "250/250 [==============================] - 0s 729us/step - loss: 0.5100 - accuracy: 0.7664 - auc: 0.8580\n",
      "Epoch 7/150\n",
      "250/250 [==============================] - 0s 734us/step - loss: 0.4845 - accuracy: 0.7836 - auc: 0.8731\n",
      "Epoch 8/150\n",
      "250/250 [==============================] - 0s 748us/step - loss: 0.4627 - accuracy: 0.7955 - auc: 0.8844\n",
      "Epoch 9/150\n",
      "250/250 [==============================] - 0s 729us/step - loss: 0.4442 - accuracy: 0.8026 - auc: 0.8926\n",
      "Epoch 10/150\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.4287 - accuracy: 0.8117 - auc: 0.8989\n",
      "Epoch 11/150\n",
      "250/250 [==============================] - 0s 727us/step - loss: 0.4142 - accuracy: 0.8195 - auc: 0.9050\n",
      "Epoch 12/150\n",
      "250/250 [==============================] - 0s 734us/step - loss: 0.4019 - accuracy: 0.8249 - auc: 0.9104\n",
      "Epoch 13/150\n",
      "250/250 [==============================] - 0s 753us/step - loss: 0.3908 - accuracy: 0.8313 - auc: 0.9153\n",
      "Epoch 14/150\n",
      "250/250 [==============================] - 0s 797us/step - loss: 0.3796 - accuracy: 0.8354 - auc: 0.9202\n",
      "Epoch 15/150\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.3689 - accuracy: 0.8413 - auc: 0.9253\n",
      "Epoch 16/150\n",
      "250/250 [==============================] - 0s 769us/step - loss: 0.3593 - accuracy: 0.8451 - auc: 0.9296\n",
      "Epoch 17/150\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.3496 - accuracy: 0.8505 - auc: 0.9339\n",
      "Epoch 18/150\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.3392 - accuracy: 0.8575 - auc: 0.9389\n",
      "Epoch 19/150\n",
      "250/250 [==============================] - 0s 788us/step - loss: 0.3293 - accuracy: 0.8610 - auc: 0.9433\n",
      "Epoch 20/150\n",
      "250/250 [==============================] - 0s 780us/step - loss: 0.3201 - accuracy: 0.8681 - auc: 0.9474\n",
      "Epoch 21/150\n",
      "250/250 [==============================] - 0s 758us/step - loss: 0.3105 - accuracy: 0.8724 - auc: 0.9514\n",
      "Epoch 22/150\n",
      "250/250 [==============================] - 0s 804us/step - loss: 0.3015 - accuracy: 0.8800 - auc: 0.9552\n",
      "Epoch 23/150\n",
      "250/250 [==============================] - 0s 723us/step - loss: 0.2923 - accuracy: 0.8850 - auc: 0.9590\n",
      "Epoch 24/150\n",
      "250/250 [==============================] - 0s 732us/step - loss: 0.2841 - accuracy: 0.8876 - auc: 0.9619\n",
      "Epoch 25/150\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.2754 - accuracy: 0.8917 - auc: 0.9653\n",
      "Epoch 26/150\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.2677 - accuracy: 0.8954 - auc: 0.9679\n",
      "Epoch 27/150\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.2600 - accuracy: 0.9031 - auc: 0.9706\n",
      "Epoch 28/150\n",
      "250/250 [==============================] - 0s 732us/step - loss: 0.2529 - accuracy: 0.9049 - auc: 0.9727\n",
      "Epoch 29/150\n",
      "250/250 [==============================] - 0s 758us/step - loss: 0.2461 - accuracy: 0.9062 - auc: 0.9746\n",
      "Epoch 30/150\n",
      "250/250 [==============================] - 0s 756us/step - loss: 0.2398 - accuracy: 0.9105 - auc: 0.9766\n",
      "Epoch 31/150\n",
      "250/250 [==============================] - 0s 852us/step - loss: 0.2336 - accuracy: 0.9143 - auc: 0.9781\n",
      "Epoch 32/150\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.2277 - accuracy: 0.9176 - auc: 0.9796\n",
      "Epoch 33/150\n",
      "250/250 [==============================] - 0s 814us/step - loss: 0.2220 - accuracy: 0.9185 - auc: 0.9809\n",
      "Epoch 34/150\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.2179 - accuracy: 0.9226 - auc: 0.9816\n",
      "Epoch 35/150\n",
      "250/250 [==============================] - 0s 848us/step - loss: 0.2128 - accuracy: 0.9234 - auc: 0.9825\n",
      "Epoch 36/150\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.2082 - accuracy: 0.9271 - auc: 0.9834\n",
      "Epoch 37/150\n",
      "250/250 [==============================] - 0s 744us/step - loss: 0.2040 - accuracy: 0.9266 - auc: 0.9840\n",
      "Epoch 38/150\n",
      "250/250 [==============================] - 0s 738us/step - loss: 0.2006 - accuracy: 0.9304 - auc: 0.9845\n",
      "Epoch 39/150\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.1969 - accuracy: 0.9314 - auc: 0.9850\n",
      "Epoch 40/150\n",
      "250/250 [==============================] - 0s 758us/step - loss: 0.1940 - accuracy: 0.9304 - auc: 0.9853\n",
      "Epoch 41/150\n",
      "250/250 [==============================] - 0s 789us/step - loss: 0.1899 - accuracy: 0.9341 - auc: 0.9859\n",
      "Epoch 42/150\n",
      "250/250 [==============================] - 0s 955us/step - loss: 0.1877 - accuracy: 0.9350 - auc: 0.9859\n",
      "Epoch 43/150\n",
      "250/250 [==============================] - 0s 749us/step - loss: 0.1851 - accuracy: 0.9350 - auc: 0.9862\n",
      "Epoch 44/150\n",
      "250/250 [==============================] - 0s 774us/step - loss: 0.1824 - accuracy: 0.9348 - auc: 0.9864\n",
      "Epoch 45/150\n",
      "250/250 [==============================] - 0s 764us/step - loss: 0.1796 - accuracy: 0.9346 - auc: 0.9868\n",
      "Epoch 46/150\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.1775 - accuracy: 0.9371 - auc: 0.9869\n",
      "Epoch 47/150\n",
      "250/250 [==============================] - 0s 752us/step - loss: 0.1754 - accuracy: 0.9370 - auc: 0.9869\n",
      "Epoch 48/150\n",
      "250/250 [==============================] - 0s 734us/step - loss: 0.1736 - accuracy: 0.9356 - auc: 0.9871\n",
      "Epoch 49/150\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.1716 - accuracy: 0.9354 - auc: 0.9873\n",
      "Epoch 50/150\n",
      "250/250 [==============================] - 0s 872us/step - loss: 0.1696 - accuracy: 0.9376 - auc: 0.9873\n",
      "Epoch 51/150\n",
      "250/250 [==============================] - 0s 778us/step - loss: 0.1681 - accuracy: 0.9370 - auc: 0.9875\n",
      "Epoch 52/150\n",
      "250/250 [==============================] - 0s 760us/step - loss: 0.1666 - accuracy: 0.9369 - auc: 0.9875\n",
      "Epoch 53/150\n",
      "250/250 [==============================] - 0s 773us/step - loss: 0.1645 - accuracy: 0.9395 - auc: 0.9878\n",
      "Epoch 54/150\n",
      "250/250 [==============================] - 0s 802us/step - loss: 0.1639 - accuracy: 0.9375 - auc: 0.9876\n",
      "Epoch 55/150\n",
      "250/250 [==============================] - 0s 764us/step - loss: 0.1619 - accuracy: 0.9379 - auc: 0.9878\n",
      "Epoch 56/150\n",
      "250/250 [==============================] - 0s 742us/step - loss: 0.1605 - accuracy: 0.9386 - auc: 0.9879\n",
      "Epoch 57/150\n",
      "250/250 [==============================] - 0s 776us/step - loss: 0.1591 - accuracy: 0.9391 - auc: 0.9880\n",
      "Epoch 58/150\n",
      "250/250 [==============================] - 0s 742us/step - loss: 0.1593 - accuracy: 0.9380 - auc: 0.9876\n",
      "Epoch 59/150\n",
      "250/250 [==============================] - 0s 773us/step - loss: 0.1575 - accuracy: 0.9395 - auc: 0.9880\n",
      "Epoch 60/150\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.1562 - accuracy: 0.9385 - auc: 0.9881\n",
      "Epoch 61/150\n",
      "250/250 [==============================] - 0s 850us/step - loss: 0.1555 - accuracy: 0.9370 - auc: 0.9880\n",
      "Epoch 62/150\n",
      "250/250 [==============================] - 0s 829us/step - loss: 0.1544 - accuracy: 0.9402 - auc: 0.9881\n",
      "Epoch 63/150\n",
      "250/250 [==============================] - 0s 813us/step - loss: 0.1538 - accuracy: 0.9390 - auc: 0.9881\n",
      "Epoch 64/150\n",
      "250/250 [==============================] - 0s 847us/step - loss: 0.1534 - accuracy: 0.9402 - auc: 0.9881\n",
      "Epoch 65/150\n",
      "250/250 [==============================] - 0s 800us/step - loss: 0.1525 - accuracy: 0.9394 - auc: 0.9880\n",
      "Epoch 66/150\n",
      "250/250 [==============================] - 0s 796us/step - loss: 0.1514 - accuracy: 0.9396 - auc: 0.9882\n",
      "Epoch 67/150\n",
      "250/250 [==============================] - 0s 757us/step - loss: 0.1507 - accuracy: 0.9384 - auc: 0.9882\n",
      "Epoch 68/150\n",
      "250/250 [==============================] - 0s 886us/step - loss: 0.1497 - accuracy: 0.9410 - auc: 0.9883\n",
      "Epoch 69/150\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.1498 - accuracy: 0.9392 - auc: 0.9882\n",
      "Epoch 70/150\n",
      "250/250 [==============================] - 0s 875us/step - loss: 0.1489 - accuracy: 0.9399 - auc: 0.9883\n",
      "Epoch 71/150\n",
      "250/250 [==============================] - 0s 812us/step - loss: 0.1478 - accuracy: 0.9401 - auc: 0.9884\n",
      "Epoch 72/150\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.1480 - accuracy: 0.9415 - auc: 0.9882\n",
      "Epoch 73/150\n",
      "250/250 [==============================] - 0s 785us/step - loss: 0.1474 - accuracy: 0.9406 - auc: 0.9883\n",
      "Epoch 74/150\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.1479 - accuracy: 0.9398 - auc: 0.9881\n",
      "Epoch 75/150\n",
      "250/250 [==============================] - 0s 786us/step - loss: 0.1469 - accuracy: 0.9400 - auc: 0.9882\n",
      "Epoch 76/150\n",
      "250/250 [==============================] - 0s 862us/step - loss: 0.1459 - accuracy: 0.9406 - auc: 0.9884\n",
      "Epoch 77/150\n",
      "250/250 [==============================] - 0s 856us/step - loss: 0.1464 - accuracy: 0.9399 - auc: 0.9882\n",
      "Epoch 78/150\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.1452 - accuracy: 0.9414 - auc: 0.9884\n",
      "Epoch 79/150\n",
      "250/250 [==============================] - 0s 845us/step - loss: 0.1450 - accuracy: 0.9421 - auc: 0.9883\n",
      "Epoch 80/150\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.1446 - accuracy: 0.9411 - auc: 0.9884\n",
      "Epoch 81/150\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.1440 - accuracy: 0.9413 - auc: 0.9885\n",
      "Epoch 82/150\n",
      "250/250 [==============================] - 0s 827us/step - loss: 0.1441 - accuracy: 0.9406 - auc: 0.9883\n",
      "Epoch 83/150\n",
      "250/250 [==============================] - 0s 819us/step - loss: 0.1438 - accuracy: 0.9392 - auc: 0.9883\n",
      "Epoch 84/150\n",
      "250/250 [==============================] - 0s 800us/step - loss: 0.1437 - accuracy: 0.9398 - auc: 0.9883\n",
      "Epoch 85/150\n",
      "250/250 [==============================] - 0s 794us/step - loss: 0.1428 - accuracy: 0.9419 - auc: 0.9885\n",
      "Epoch 86/150\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.1428 - accuracy: 0.9413 - auc: 0.9884\n",
      "Epoch 87/150\n",
      "250/250 [==============================] - 0s 778us/step - loss: 0.1431 - accuracy: 0.9402 - auc: 0.9882\n",
      "Epoch 88/150\n",
      "250/250 [==============================] - 0s 774us/step - loss: 0.1428 - accuracy: 0.9408 - auc: 0.9883\n",
      "Epoch 89/150\n",
      "250/250 [==============================] - 0s 888us/step - loss: 0.1414 - accuracy: 0.9421 - auc: 0.9885\n",
      "Epoch 90/150\n",
      "250/250 [==============================] - 0s 892us/step - loss: 0.1419 - accuracy: 0.9415 - auc: 0.9884\n",
      "Epoch 91/150\n",
      "250/250 [==============================] - 0s 826us/step - loss: 0.1421 - accuracy: 0.9398 - auc: 0.9884\n",
      "Epoch 92/150\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.1418 - accuracy: 0.9396 - auc: 0.9883\n",
      "Epoch 93/150\n",
      "250/250 [==============================] - 0s 781us/step - loss: 0.1418 - accuracy: 0.9404 - auc: 0.9883\n",
      "Epoch 94/150\n",
      "250/250 [==============================] - 0s 816us/step - loss: 0.1410 - accuracy: 0.9410 - auc: 0.9884\n",
      "Epoch 95/150\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.1409 - accuracy: 0.9398 - auc: 0.9885\n",
      "Epoch 96/150\n",
      "250/250 [==============================] - 0s 785us/step - loss: 0.1410 - accuracy: 0.9409 - auc: 0.9884\n",
      "Epoch 97/150\n",
      "250/250 [==============================] - 0s 796us/step - loss: 0.1406 - accuracy: 0.9411 - auc: 0.9884\n",
      "Epoch 98/150\n",
      "250/250 [==============================] - 0s 905us/step - loss: 0.1406 - accuracy: 0.9405 - auc: 0.9884\n",
      "Epoch 99/150\n",
      "250/250 [==============================] - 0s 926us/step - loss: 0.1408 - accuracy: 0.9413 - auc: 0.9884\n",
      "Epoch 100/150\n",
      "250/250 [==============================] - 0s 801us/step - loss: 0.1409 - accuracy: 0.9398 - auc: 0.9883\n",
      "Epoch 101/150\n",
      "250/250 [==============================] - 0s 741us/step - loss: 0.1406 - accuracy: 0.9394 - auc: 0.9884\n",
      "Epoch 102/150\n",
      "250/250 [==============================] - 0s 769us/step - loss: 0.1403 - accuracy: 0.9396 - auc: 0.9884\n",
      "Epoch 103/150\n",
      "250/250 [==============================] - 0s 784us/step - loss: 0.1406 - accuracy: 0.9391 - auc: 0.9883\n",
      "Epoch 104/150\n",
      "250/250 [==============================] - 0s 989us/step - loss: 0.1400 - accuracy: 0.9399 - auc: 0.9884\n",
      "Epoch 105/150\n",
      "250/250 [==============================] - 0s 864us/step - loss: 0.1397 - accuracy: 0.9425 - auc: 0.9885\n",
      "Epoch 106/150\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9408 - auc: 0.9887\n",
      "Epoch 107/150\n",
      "250/250 [==============================] - 0s 976us/step - loss: 0.1394 - accuracy: 0.9409 - auc: 0.9885\n",
      "Epoch 108/150\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9419 - auc: 0.9885\n",
      "Epoch 109/150\n",
      "250/250 [==============================] - 0s 925us/step - loss: 0.1399 - accuracy: 0.9413 - auc: 0.9884\n",
      "Epoch 110/150\n",
      "250/250 [==============================] - 0s 882us/step - loss: 0.1393 - accuracy: 0.9414 - auc: 0.9885\n",
      "Epoch 111/150\n",
      "250/250 [==============================] - 0s 880us/step - loss: 0.1390 - accuracy: 0.9408 - auc: 0.9885\n",
      "Epoch 112/150\n",
      "250/250 [==============================] - 0s 913us/step - loss: 0.1390 - accuracy: 0.9413 - auc: 0.9885\n",
      "Epoch 113/150\n",
      "250/250 [==============================] - 0s 872us/step - loss: 0.1399 - accuracy: 0.9409 - auc: 0.9883\n",
      "Epoch 114/150\n",
      "250/250 [==============================] - 0s 821us/step - loss: 0.1395 - accuracy: 0.9410 - auc: 0.9884\n",
      "Epoch 115/150\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.1394 - accuracy: 0.9399 - auc: 0.9884\n",
      "Epoch 116/150\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.1393 - accuracy: 0.9414 - auc: 0.9884\n",
      "Epoch 117/150\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.1388 - accuracy: 0.9423 - auc: 0.9885\n",
      "Epoch 118/150\n",
      "250/250 [==============================] - 0s 780us/step - loss: 0.1395 - accuracy: 0.9404 - auc: 0.9883\n",
      "Epoch 119/150\n",
      "250/250 [==============================] - 0s 786us/step - loss: 0.1392 - accuracy: 0.9417 - auc: 0.9884\n",
      "Epoch 120/150\n",
      "250/250 [==============================] - 0s 874us/step - loss: 0.1389 - accuracy: 0.9402 - auc: 0.9884\n",
      "Epoch 121/150\n",
      "250/250 [==============================] - 0s 782us/step - loss: 0.1399 - accuracy: 0.9408 - auc: 0.9882\n",
      "Epoch 122/150\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.1391 - accuracy: 0.9396 - auc: 0.9883\n",
      "Epoch 123/150\n",
      "250/250 [==============================] - 0s 848us/step - loss: 0.1390 - accuracy: 0.9410 - auc: 0.9884\n",
      "Epoch 124/150\n",
      "250/250 [==============================] - 0s 829us/step - loss: 0.1389 - accuracy: 0.9384 - auc: 0.9884\n",
      "Epoch 125/150\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.1390 - accuracy: 0.9400 - auc: 0.9884\n",
      "Epoch 126/150\n",
      "250/250 [==============================] - 0s 835us/step - loss: 0.1380 - accuracy: 0.9409 - auc: 0.9885\n",
      "Epoch 127/150\n",
      "250/250 [==============================] - 0s 796us/step - loss: 0.1377 - accuracy: 0.9421 - auc: 0.9886\n",
      "Epoch 128/150\n",
      "250/250 [==============================] - 0s 800us/step - loss: 0.1388 - accuracy: 0.9416 - auc: 0.9884\n",
      "Epoch 129/150\n",
      "250/250 [==============================] - 0s 875us/step - loss: 0.1390 - accuracy: 0.9406 - auc: 0.9883\n",
      "Epoch 130/150\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.1392 - accuracy: 0.9389 - auc: 0.9883\n",
      "Epoch 131/150\n",
      "250/250 [==============================] - 0s 786us/step - loss: 0.1382 - accuracy: 0.9420 - auc: 0.9885\n",
      "Epoch 132/150\n",
      "250/250 [==============================] - 0s 778us/step - loss: 0.1384 - accuracy: 0.9400 - auc: 0.9884\n",
      "Epoch 133/150\n",
      "250/250 [==============================] - 0s 778us/step - loss: 0.1386 - accuracy: 0.9425 - auc: 0.9884\n",
      "Epoch 134/150\n",
      "250/250 [==============================] - 0s 778us/step - loss: 0.1391 - accuracy: 0.9396 - auc: 0.9883\n",
      "Epoch 135/150\n",
      "250/250 [==============================] - 0s 896us/step - loss: 0.1383 - accuracy: 0.9399 - auc: 0.9884\n",
      "Epoch 136/150\n",
      "250/250 [==============================] - 0s 804us/step - loss: 0.1378 - accuracy: 0.9421 - auc: 0.9886\n",
      "Epoch 137/150\n",
      "250/250 [==============================] - 0s 784us/step - loss: 0.1379 - accuracy: 0.9406 - auc: 0.9885\n",
      "Epoch 138/150\n",
      "250/250 [==============================] - 0s 769us/step - loss: 0.1380 - accuracy: 0.9408 - auc: 0.9885\n",
      "Epoch 139/150\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.1381 - accuracy: 0.9408 - auc: 0.9885\n",
      "Epoch 140/150\n",
      "250/250 [==============================] - 0s 774us/step - loss: 0.1387 - accuracy: 0.9396 - auc: 0.9883\n",
      "Epoch 141/150\n",
      "250/250 [==============================] - 0s 774us/step - loss: 0.1379 - accuracy: 0.9419 - auc: 0.9885\n",
      "Epoch 142/150\n",
      "250/250 [==============================] - 0s 750us/step - loss: 0.1377 - accuracy: 0.9409 - auc: 0.9885\n",
      "Epoch 143/150\n",
      "250/250 [==============================] - 0s 770us/step - loss: 0.1386 - accuracy: 0.9396 - auc: 0.9884\n",
      "Epoch 144/150\n",
      "250/250 [==============================] - 0s 774us/step - loss: 0.1379 - accuracy: 0.9404 - auc: 0.9884\n",
      "Epoch 145/150\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.1387 - accuracy: 0.9409 - auc: 0.9883\n",
      "Epoch 146/150\n",
      "250/250 [==============================] - 0s 777us/step - loss: 0.1375 - accuracy: 0.9419 - auc: 0.9886\n",
      "Epoch 147/150\n",
      "250/250 [==============================] - 0s 765us/step - loss: 0.1378 - accuracy: 0.9414 - auc: 0.9885\n",
      "Epoch 148/150\n",
      "250/250 [==============================] - 0s 758us/step - loss: 0.1378 - accuracy: 0.9406 - auc: 0.9885\n",
      "Epoch 149/150\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.1378 - accuracy: 0.9395 - auc: 0.9885\n",
      "Epoch 150/150\n",
      "250/250 [==============================] - 0s 754us/step - loss: 0.1378 - accuracy: 0.9411 - auc: 0.9885\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(scaled_train, train_y, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "a8c9881c-9c1f-4e15-b540-36fdab12bbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = model.predict(scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "3eab4b29-d9b3-4c22-8c85-b2a00aba3742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.04163015\n",
      "RMSE:  0.20403467\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"MSE: \", metrics.mean_squared_error(test_y, test_prediction))\n",
    "print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(test_y, test_prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165c882-85b3-4951-882a-04106467083b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
