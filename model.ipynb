{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "bf0f4eca-2474-450b-813e-49cb36348979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "0a81611a-cf67-4e22-a591-12f8470f85a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.881108</td>\n",
       "      <td>3.616019</td>\n",
       "      <td>3.601147</td>\n",
       "      <td>3.417457</td>\n",
       "      <td>2.723198</td>\n",
       "      <td>3.330895</td>\n",
       "      <td>3.723629</td>\n",
       "      <td>3.709737</td>\n",
       "      <td>5.848922</td>\n",
       "      <td>5.252491</td>\n",
       "      <td>...</td>\n",
       "      <td>6.215646</td>\n",
       "      <td>6.200070</td>\n",
       "      <td>5.842163</td>\n",
       "      <td>6.148899</td>\n",
       "      <td>5.333915</td>\n",
       "      <td>5.385360</td>\n",
       "      <td>4.851503</td>\n",
       "      <td>5.118068</td>\n",
       "      <td>5.113795</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.800883</td>\n",
       "      <td>3.210280</td>\n",
       "      <td>3.081142</td>\n",
       "      <td>3.498283</td>\n",
       "      <td>2.734601</td>\n",
       "      <td>3.105662</td>\n",
       "      <td>3.801500</td>\n",
       "      <td>3.796322</td>\n",
       "      <td>5.062057</td>\n",
       "      <td>4.987285</td>\n",
       "      <td>...</td>\n",
       "      <td>6.573781</td>\n",
       "      <td>5.749618</td>\n",
       "      <td>6.747027</td>\n",
       "      <td>6.537529</td>\n",
       "      <td>5.186155</td>\n",
       "      <td>5.420567</td>\n",
       "      <td>4.777363</td>\n",
       "      <td>5.120788</td>\n",
       "      <td>5.136329</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.964203</td>\n",
       "      <td>3.526702</td>\n",
       "      <td>3.255776</td>\n",
       "      <td>3.002896</td>\n",
       "      <td>2.812806</td>\n",
       "      <td>3.133526</td>\n",
       "      <td>3.635403</td>\n",
       "      <td>3.581440</td>\n",
       "      <td>5.607975</td>\n",
       "      <td>4.847624</td>\n",
       "      <td>...</td>\n",
       "      <td>6.162477</td>\n",
       "      <td>5.844224</td>\n",
       "      <td>6.073829</td>\n",
       "      <td>6.075009</td>\n",
       "      <td>5.572521</td>\n",
       "      <td>5.194047</td>\n",
       "      <td>4.771206</td>\n",
       "      <td>5.424286</td>\n",
       "      <td>5.628824</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.907543</td>\n",
       "      <td>3.201577</td>\n",
       "      <td>3.527606</td>\n",
       "      <td>2.861318</td>\n",
       "      <td>2.670537</td>\n",
       "      <td>3.311229</td>\n",
       "      <td>3.855523</td>\n",
       "      <td>3.877643</td>\n",
       "      <td>5.054537</td>\n",
       "      <td>4.982864</td>\n",
       "      <td>...</td>\n",
       "      <td>5.755873</td>\n",
       "      <td>5.531023</td>\n",
       "      <td>5.888425</td>\n",
       "      <td>6.627419</td>\n",
       "      <td>6.000076</td>\n",
       "      <td>5.059838</td>\n",
       "      <td>4.898310</td>\n",
       "      <td>5.887313</td>\n",
       "      <td>5.005183</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.051307</td>\n",
       "      <td>3.730421</td>\n",
       "      <td>3.192650</td>\n",
       "      <td>3.007943</td>\n",
       "      <td>2.954206</td>\n",
       "      <td>3.994215</td>\n",
       "      <td>3.670745</td>\n",
       "      <td>3.922720</td>\n",
       "      <td>5.561307</td>\n",
       "      <td>5.343242</td>\n",
       "      <td>...</td>\n",
       "      <td>5.712020</td>\n",
       "      <td>5.931136</td>\n",
       "      <td>6.635594</td>\n",
       "      <td>6.164542</td>\n",
       "      <td>5.439445</td>\n",
       "      <td>5.488501</td>\n",
       "      <td>4.781580</td>\n",
       "      <td>5.307637</td>\n",
       "      <td>5.480369</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>4.441229</td>\n",
       "      <td>4.115273</td>\n",
       "      <td>3.472402</td>\n",
       "      <td>2.745761</td>\n",
       "      <td>3.660351</td>\n",
       "      <td>4.093115</td>\n",
       "      <td>4.007465</td>\n",
       "      <td>4.291413</td>\n",
       "      <td>5.070075</td>\n",
       "      <td>5.110675</td>\n",
       "      <td>...</td>\n",
       "      <td>5.678653</td>\n",
       "      <td>5.965473</td>\n",
       "      <td>6.742951</td>\n",
       "      <td>6.404496</td>\n",
       "      <td>5.362002</td>\n",
       "      <td>5.981578</td>\n",
       "      <td>5.556540</td>\n",
       "      <td>5.991490</td>\n",
       "      <td>5.850924</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>4.016708</td>\n",
       "      <td>3.746175</td>\n",
       "      <td>3.199771</td>\n",
       "      <td>3.178246</td>\n",
       "      <td>3.459837</td>\n",
       "      <td>3.968765</td>\n",
       "      <td>4.398305</td>\n",
       "      <td>4.565947</td>\n",
       "      <td>5.133747</td>\n",
       "      <td>5.116910</td>\n",
       "      <td>...</td>\n",
       "      <td>6.506161</td>\n",
       "      <td>5.920921</td>\n",
       "      <td>6.675199</td>\n",
       "      <td>6.688912</td>\n",
       "      <td>5.470335</td>\n",
       "      <td>6.004166</td>\n",
       "      <td>5.682725</td>\n",
       "      <td>5.648218</td>\n",
       "      <td>5.464242</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>3.937626</td>\n",
       "      <td>3.803369</td>\n",
       "      <td>3.818526</td>\n",
       "      <td>3.293163</td>\n",
       "      <td>3.354351</td>\n",
       "      <td>3.720848</td>\n",
       "      <td>3.730639</td>\n",
       "      <td>4.349628</td>\n",
       "      <td>5.163885</td>\n",
       "      <td>4.562440</td>\n",
       "      <td>...</td>\n",
       "      <td>6.393017</td>\n",
       "      <td>5.727222</td>\n",
       "      <td>6.121406</td>\n",
       "      <td>6.107012</td>\n",
       "      <td>5.998606</td>\n",
       "      <td>6.053508</td>\n",
       "      <td>5.697338</td>\n",
       "      <td>5.854592</td>\n",
       "      <td>5.457969</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>4.252633</td>\n",
       "      <td>3.560719</td>\n",
       "      <td>3.353298</td>\n",
       "      <td>3.695314</td>\n",
       "      <td>3.451712</td>\n",
       "      <td>3.377892</td>\n",
       "      <td>4.307202</td>\n",
       "      <td>4.082620</td>\n",
       "      <td>5.769166</td>\n",
       "      <td>5.403883</td>\n",
       "      <td>...</td>\n",
       "      <td>6.438657</td>\n",
       "      <td>6.272212</td>\n",
       "      <td>6.747316</td>\n",
       "      <td>6.907396</td>\n",
       "      <td>5.848778</td>\n",
       "      <td>5.813068</td>\n",
       "      <td>5.720781</td>\n",
       "      <td>5.539517</td>\n",
       "      <td>5.741377</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>4.425644</td>\n",
       "      <td>4.136055</td>\n",
       "      <td>3.789301</td>\n",
       "      <td>3.395030</td>\n",
       "      <td>3.550975</td>\n",
       "      <td>4.012016</td>\n",
       "      <td>4.102223</td>\n",
       "      <td>4.446715</td>\n",
       "      <td>5.587647</td>\n",
       "      <td>5.135701</td>\n",
       "      <td>...</td>\n",
       "      <td>5.875079</td>\n",
       "      <td>6.124633</td>\n",
       "      <td>6.212417</td>\n",
       "      <td>6.455535</td>\n",
       "      <td>5.900350</td>\n",
       "      <td>5.848514</td>\n",
       "      <td>5.736068</td>\n",
       "      <td>5.372799</td>\n",
       "      <td>5.439422</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     3.881108  3.616019  3.601147  3.417457  2.723198  3.330895  3.723629   \n",
       "1     3.800883  3.210280  3.081142  3.498283  2.734601  3.105662  3.801500   \n",
       "2     3.964203  3.526702  3.255776  3.002896  2.812806  3.133526  3.635403   \n",
       "3     3.907543  3.201577  3.527606  2.861318  2.670537  3.311229  3.855523   \n",
       "4     4.051307  3.730421  3.192650  3.007943  2.954206  3.994215  3.670745   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995  4.441229  4.115273  3.472402  2.745761  3.660351  4.093115  4.007465   \n",
       "9996  4.016708  3.746175  3.199771  3.178246  3.459837  3.968765  4.398305   \n",
       "9997  3.937626  3.803369  3.818526  3.293163  3.354351  3.720848  3.730639   \n",
       "9998  4.252633  3.560719  3.353298  3.695314  3.451712  3.377892  4.307202   \n",
       "9999  4.425644  4.136055  3.789301  3.395030  3.550975  4.012016  4.102223   \n",
       "\n",
       "             7         8         9  ...        15        16        17  \\\n",
       "0     3.709737  5.848922  5.252491  ...  6.215646  6.200070  5.842163   \n",
       "1     3.796322  5.062057  4.987285  ...  6.573781  5.749618  6.747027   \n",
       "2     3.581440  5.607975  4.847624  ...  6.162477  5.844224  6.073829   \n",
       "3     3.877643  5.054537  4.982864  ...  5.755873  5.531023  5.888425   \n",
       "4     3.922720  5.561307  5.343242  ...  5.712020  5.931136  6.635594   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9995  4.291413  5.070075  5.110675  ...  5.678653  5.965473  6.742951   \n",
       "9996  4.565947  5.133747  5.116910  ...  6.506161  5.920921  6.675199   \n",
       "9997  4.349628  5.163885  4.562440  ...  6.393017  5.727222  6.121406   \n",
       "9998  4.082620  5.769166  5.403883  ...  6.438657  6.272212  6.747316   \n",
       "9999  4.446715  5.587647  5.135701  ...  5.875079  6.124633  6.212417   \n",
       "\n",
       "            18        19        20        21        22        23  label  \n",
       "0     6.148899  5.333915  5.385360  4.851503  5.118068  5.113795   True  \n",
       "1     6.537529  5.186155  5.420567  4.777363  5.120788  5.136329   True  \n",
       "2     6.075009  5.572521  5.194047  4.771206  5.424286  5.628824   True  \n",
       "3     6.627419  6.000076  5.059838  4.898310  5.887313  5.005183   True  \n",
       "4     6.164542  5.439445  5.488501  4.781580  5.307637  5.480369   True  \n",
       "...        ...       ...       ...       ...       ...       ...    ...  \n",
       "9995  6.404496  5.362002  5.981578  5.556540  5.991490  5.850924  False  \n",
       "9996  6.688912  5.470335  6.004166  5.682725  5.648218  5.464242  False  \n",
       "9997  6.107012  5.998606  6.053508  5.697338  5.854592  5.457969  False  \n",
       "9998  6.907396  5.848778  5.813068  5.720781  5.539517  5.741377  False  \n",
       "9999  6.455535  5.900350  5.848514  5.736068  5.372799  5.439422  False  \n",
       "\n",
       "[10000 rows x 25 columns]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read training data sets,\n",
    "\n",
    "hours = list(map(str, range(24))) + ['label']\n",
    "prices = pd.read_csv('./TrainingData.csv', names = hours,  converters = {'label': lambda x: not bool(int(x))})\n",
    "\n",
    "\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e3bea-43aa-4321-b37a-c8acbb7a597d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "27e4378a-32ae-4b2c-a18a-f7936fe09bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = prices.sample(frac=0.8, random_state=10)\n",
    "train_y = train_x.pop('label')\n",
    "\n",
    "\n",
    "test_x = prices.drop(train_x.index)\n",
    "test_y = test_x.pop('label')\n",
    "\n",
    "# scale based on the training set\n",
    "scaler = MinMaxScaler()\n",
    "scaled_train = scaler.fit_transform(train_x)\n",
    "scaled_test = scaler.fit_transform(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "ed6c550e-3762-4dfe-b94a-be6506cb61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "b0243ca3-8fcc-4cb2-98e5-8dc79fc5f501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_43 (Dense)            (None, 1)                 25        \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(tf.keras.layers.Dense(1, input_shape=(24,)))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "3f4070f1-4f71-4294-9eac-030f156de67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "3058936d-75a8-48ce-8031-d45d56c471e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.7208 - accuracy: 0.5155\n",
      "Epoch 2/150\n",
      "250/250 [==============================] - 0s 823us/step - loss: 0.6793 - accuracy: 0.5646\n",
      "Epoch 3/150\n",
      "250/250 [==============================] - 0s 675us/step - loss: 0.6515 - accuracy: 0.6233\n",
      "Epoch 4/150\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.6199 - accuracy: 0.6765\n",
      "Epoch 5/150\n",
      "250/250 [==============================] - 0s 659us/step - loss: 0.5826 - accuracy: 0.7191\n",
      "Epoch 6/150\n",
      "250/250 [==============================] - 0s 660us/step - loss: 0.5433 - accuracy: 0.7530\n",
      "Epoch 7/150\n",
      "250/250 [==============================] - 0s 665us/step - loss: 0.5076 - accuracy: 0.7769\n",
      "Epoch 8/150\n",
      "250/250 [==============================] - 0s 665us/step - loss: 0.4778 - accuracy: 0.7903\n",
      "Epoch 9/150\n",
      "250/250 [==============================] - 0s 659us/step - loss: 0.4538 - accuracy: 0.8005\n",
      "Epoch 10/150\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.4344 - accuracy: 0.8091\n",
      "Epoch 11/150\n",
      "250/250 [==============================] - 0s 669us/step - loss: 0.4184 - accuracy: 0.8156\n",
      "Epoch 12/150\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.4048 - accuracy: 0.8251\n",
      "Epoch 13/150\n",
      "250/250 [==============================] - 0s 663us/step - loss: 0.3930 - accuracy: 0.8273\n",
      "Epoch 14/150\n",
      "250/250 [==============================] - 0s 688us/step - loss: 0.3817 - accuracy: 0.8342\n",
      "Epoch 15/150\n",
      "250/250 [==============================] - 0s 661us/step - loss: 0.3709 - accuracy: 0.8379\n",
      "Epoch 16/150\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.3606 - accuracy: 0.8453\n",
      "Epoch 17/150\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.3501 - accuracy: 0.8505\n",
      "Epoch 18/150\n",
      "250/250 [==============================] - 0s 671us/step - loss: 0.3395 - accuracy: 0.8554\n",
      "Epoch 19/150\n",
      "250/250 [==============================] - 0s 681us/step - loss: 0.3301 - accuracy: 0.8606\n",
      "Epoch 20/150\n",
      "250/250 [==============================] - 0s 672us/step - loss: 0.3200 - accuracy: 0.8705\n",
      "Epoch 21/150\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.3108 - accuracy: 0.8709\n",
      "Epoch 22/150\n",
      "250/250 [==============================] - 0s 719us/step - loss: 0.3012 - accuracy: 0.8767\n",
      "Epoch 23/150\n",
      "250/250 [==============================] - 0s 784us/step - loss: 0.2931 - accuracy: 0.8834\n",
      "Epoch 24/150\n",
      "250/250 [==============================] - 0s 687us/step - loss: 0.2839 - accuracy: 0.8891\n",
      "Epoch 25/150\n",
      "250/250 [==============================] - 0s 680us/step - loss: 0.2756 - accuracy: 0.8935\n",
      "Epoch 26/150\n",
      "250/250 [==============================] - 0s 707us/step - loss: 0.2678 - accuracy: 0.8954\n",
      "Epoch 27/150\n",
      "250/250 [==============================] - 0s 695us/step - loss: 0.2603 - accuracy: 0.9004\n",
      "Epoch 28/150\n",
      "250/250 [==============================] - 0s 720us/step - loss: 0.2525 - accuracy: 0.9031\n",
      "Epoch 29/150\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.2457 - accuracy: 0.9101\n",
      "Epoch 30/150\n",
      "250/250 [==============================] - 0s 665us/step - loss: 0.2392 - accuracy: 0.9106\n",
      "Epoch 31/150\n",
      "250/250 [==============================] - 0s 664us/step - loss: 0.2331 - accuracy: 0.9133\n",
      "Epoch 32/150\n",
      "250/250 [==============================] - 0s 695us/step - loss: 0.2273 - accuracy: 0.9169\n",
      "Epoch 33/150\n",
      "250/250 [==============================] - 0s 683us/step - loss: 0.2217 - accuracy: 0.9190\n",
      "Epoch 34/150\n",
      "250/250 [==============================] - 0s 671us/step - loss: 0.2166 - accuracy: 0.9216\n",
      "Epoch 35/150\n",
      "250/250 [==============================] - 0s 684us/step - loss: 0.2124 - accuracy: 0.9246\n",
      "Epoch 36/150\n",
      "250/250 [==============================] - 0s 679us/step - loss: 0.2076 - accuracy: 0.9247\n",
      "Epoch 37/150\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.2038 - accuracy: 0.9281\n",
      "Epoch 38/150\n",
      "250/250 [==============================] - 0s 695us/step - loss: 0.1995 - accuracy: 0.9311\n",
      "Epoch 39/150\n",
      "250/250 [==============================] - 0s 749us/step - loss: 0.1963 - accuracy: 0.9291\n",
      "Epoch 40/150\n",
      "250/250 [==============================] - 0s 681us/step - loss: 0.1927 - accuracy: 0.9335\n",
      "Epoch 41/150\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.1897 - accuracy: 0.9314\n",
      "Epoch 42/150\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.1862 - accuracy: 0.9348\n",
      "Epoch 43/150\n",
      "250/250 [==============================] - 0s 675us/step - loss: 0.1840 - accuracy: 0.9329\n",
      "Epoch 44/150\n",
      "250/250 [==============================] - 0s 687us/step - loss: 0.1812 - accuracy: 0.9366\n",
      "Epoch 45/150\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.1785 - accuracy: 0.9365\n",
      "Epoch 46/150\n",
      "250/250 [==============================] - 0s 679us/step - loss: 0.1767 - accuracy: 0.9350\n",
      "Epoch 47/150\n",
      "250/250 [==============================] - 0s 693us/step - loss: 0.1745 - accuracy: 0.9362\n",
      "Epoch 48/150\n",
      "250/250 [==============================] - 0s 715us/step - loss: 0.1731 - accuracy: 0.9352\n",
      "Epoch 49/150\n",
      "250/250 [==============================] - 0s 721us/step - loss: 0.1709 - accuracy: 0.9376\n",
      "Epoch 50/150\n",
      "250/250 [==============================] - 0s 724us/step - loss: 0.1691 - accuracy: 0.9367\n",
      "Epoch 51/150\n",
      "250/250 [==============================] - 0s 758us/step - loss: 0.1672 - accuracy: 0.9370\n",
      "Epoch 52/150\n",
      "250/250 [==============================] - 0s 693us/step - loss: 0.1663 - accuracy: 0.9362\n",
      "Epoch 53/150\n",
      "250/250 [==============================] - 0s 699us/step - loss: 0.1637 - accuracy: 0.9394\n",
      "Epoch 54/150\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.1625 - accuracy: 0.9399\n",
      "Epoch 55/150\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.1612 - accuracy: 0.9394\n",
      "Epoch 56/150\n",
      "250/250 [==============================] - 0s 691us/step - loss: 0.1596 - accuracy: 0.9396\n",
      "Epoch 57/150\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.1585 - accuracy: 0.9389\n",
      "Epoch 58/150\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.1577 - accuracy: 0.9399\n",
      "Epoch 59/150\n",
      "250/250 [==============================] - 0s 750us/step - loss: 0.1568 - accuracy: 0.9391\n",
      "Epoch 60/150\n",
      "250/250 [==============================] - 0s 721us/step - loss: 0.1556 - accuracy: 0.9401\n",
      "Epoch 61/150\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.1555 - accuracy: 0.9384\n",
      "Epoch 62/150\n",
      "250/250 [==============================] - 0s 749us/step - loss: 0.1539 - accuracy: 0.9379\n",
      "Epoch 63/150\n",
      "250/250 [==============================] - 0s 681us/step - loss: 0.1532 - accuracy: 0.9404\n",
      "Epoch 64/150\n",
      "250/250 [==============================] - 0s 689us/step - loss: 0.1526 - accuracy: 0.9408\n",
      "Epoch 65/150\n",
      "250/250 [==============================] - 0s 677us/step - loss: 0.1516 - accuracy: 0.9398\n",
      "Epoch 66/150\n",
      "250/250 [==============================] - 0s 866us/step - loss: 0.1517 - accuracy: 0.9402\n",
      "Epoch 67/150\n",
      "250/250 [==============================] - 0s 745us/step - loss: 0.1502 - accuracy: 0.9400\n",
      "Epoch 68/150\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.1495 - accuracy: 0.9413\n",
      "Epoch 69/150\n",
      "250/250 [==============================] - 0s 707us/step - loss: 0.1492 - accuracy: 0.9391\n",
      "Epoch 70/150\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.1489 - accuracy: 0.9395\n",
      "Epoch 71/150\n",
      "250/250 [==============================] - 0s 693us/step - loss: 0.1477 - accuracy: 0.9404\n",
      "Epoch 72/150\n",
      "250/250 [==============================] - 0s 692us/step - loss: 0.1471 - accuracy: 0.9426\n",
      "Epoch 73/150\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.1471 - accuracy: 0.9405\n",
      "Epoch 74/150\n",
      "250/250 [==============================] - 0s 677us/step - loss: 0.1467 - accuracy: 0.9410\n",
      "Epoch 75/150\n",
      "250/250 [==============================] - 0s 704us/step - loss: 0.1461 - accuracy: 0.9405\n",
      "Epoch 76/150\n",
      "250/250 [==============================] - 0s 697us/step - loss: 0.1460 - accuracy: 0.9409\n",
      "Epoch 77/150\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.1458 - accuracy: 0.9406\n",
      "Epoch 78/150\n",
      "250/250 [==============================] - 0s 689us/step - loss: 0.1448 - accuracy: 0.9424\n",
      "Epoch 79/150\n",
      "250/250 [==============================] - 0s 687us/step - loss: 0.1445 - accuracy: 0.9417\n",
      "Epoch 80/150\n",
      "250/250 [==============================] - 0s 703us/step - loss: 0.1444 - accuracy: 0.9413\n",
      "Epoch 81/150\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.1440 - accuracy: 0.9411\n",
      "Epoch 82/150\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.1433 - accuracy: 0.9404\n",
      "Epoch 83/150\n",
      "250/250 [==============================] - 0s 679us/step - loss: 0.1436 - accuracy: 0.9402\n",
      "Epoch 84/150\n",
      "250/250 [==============================] - 0s 715us/step - loss: 0.1428 - accuracy: 0.9410\n",
      "Epoch 85/150\n",
      "250/250 [==============================] - 0s 687us/step - loss: 0.1433 - accuracy: 0.9410\n",
      "Epoch 86/150\n",
      "250/250 [==============================] - 0s 703us/step - loss: 0.1421 - accuracy: 0.9435\n",
      "Epoch 87/150\n",
      "250/250 [==============================] - 0s 696us/step - loss: 0.1419 - accuracy: 0.9401\n",
      "Epoch 88/150\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.1417 - accuracy: 0.9424\n",
      "Epoch 89/150\n",
      "250/250 [==============================] - 0s 749us/step - loss: 0.1424 - accuracy: 0.9396\n",
      "Epoch 90/150\n",
      "250/250 [==============================] - 0s 850us/step - loss: 0.1417 - accuracy: 0.9398\n",
      "Epoch 91/150\n",
      "250/250 [==============================] - 0s 724us/step - loss: 0.1417 - accuracy: 0.9424\n",
      "Epoch 92/150\n",
      "250/250 [==============================] - 0s 703us/step - loss: 0.1420 - accuracy: 0.9419\n",
      "Epoch 93/150\n",
      "250/250 [==============================] - 0s 709us/step - loss: 0.1409 - accuracy: 0.9401\n",
      "Epoch 94/150\n",
      "250/250 [==============================] - 0s 703us/step - loss: 0.1405 - accuracy: 0.9408\n",
      "Epoch 95/150\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.1411 - accuracy: 0.9413\n",
      "Epoch 96/150\n",
      "250/250 [==============================] - 0s 671us/step - loss: 0.1411 - accuracy: 0.9396\n",
      "Epoch 97/150\n",
      "250/250 [==============================] - 0s 703us/step - loss: 0.1406 - accuracy: 0.9391\n",
      "Epoch 98/150\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.1403 - accuracy: 0.9402\n",
      "Epoch 99/150\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.1407 - accuracy: 0.9405\n",
      "Epoch 100/150\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.1402 - accuracy: 0.9415\n",
      "Epoch 101/150\n",
      "250/250 [==============================] - 0s 724us/step - loss: 0.1402 - accuracy: 0.9404\n",
      "Epoch 102/150\n",
      "250/250 [==============================] - 0s 973us/step - loss: 0.1402 - accuracy: 0.9405\n",
      "Epoch 103/150\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.1396 - accuracy: 0.9416\n",
      "Epoch 104/150\n",
      "250/250 [==============================] - 0s 831us/step - loss: 0.1398 - accuracy: 0.9404\n",
      "Epoch 105/150\n",
      "250/250 [==============================] - 0s 745us/step - loss: 0.1412 - accuracy: 0.9388\n",
      "Epoch 106/150\n",
      "250/250 [==============================] - 0s 644us/step - loss: 0.1398 - accuracy: 0.9389\n",
      "Epoch 107/150\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.1396 - accuracy: 0.9415\n",
      "Epoch 108/150\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.1394 - accuracy: 0.9408\n",
      "Epoch 109/150\n",
      "250/250 [==============================] - 0s 636us/step - loss: 0.1393 - accuracy: 0.9417\n",
      "Epoch 110/150\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.1384 - accuracy: 0.9421\n",
      "Epoch 111/150\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.1404 - accuracy: 0.9386\n",
      "Epoch 112/150\n",
      "250/250 [==============================] - 0s 648us/step - loss: 0.1389 - accuracy: 0.9423\n",
      "Epoch 113/150\n",
      "250/250 [==============================] - 0s 632us/step - loss: 0.1390 - accuracy: 0.9409\n",
      "Epoch 114/150\n",
      "250/250 [==============================] - 0s 631us/step - loss: 0.1390 - accuracy: 0.9415\n",
      "Epoch 115/150\n",
      "250/250 [==============================] - 0s 644us/step - loss: 0.1387 - accuracy: 0.9424\n",
      "Epoch 116/150\n",
      "250/250 [==============================] - 0s 632us/step - loss: 0.1406 - accuracy: 0.9391\n",
      "Epoch 117/150\n",
      "250/250 [==============================] - 0s 632us/step - loss: 0.1392 - accuracy: 0.9385\n",
      "Epoch 118/150\n",
      "250/250 [==============================] - 0s 635us/step - loss: 0.1391 - accuracy: 0.9394\n",
      "Epoch 119/150\n",
      "250/250 [==============================] - 0s 633us/step - loss: 0.1393 - accuracy: 0.9414\n",
      "Epoch 120/150\n",
      "250/250 [==============================] - 0s 665us/step - loss: 0.1396 - accuracy: 0.9415\n",
      "Epoch 121/150\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.1388 - accuracy: 0.9394\n",
      "Epoch 122/150\n",
      "250/250 [==============================] - 0s 665us/step - loss: 0.1393 - accuracy: 0.9401\n",
      "Epoch 123/150\n",
      "250/250 [==============================] - 0s 644us/step - loss: 0.1389 - accuracy: 0.9396\n",
      "Epoch 124/150\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.1390 - accuracy: 0.9425\n",
      "Epoch 125/150\n",
      "250/250 [==============================] - 0s 661us/step - loss: 0.1387 - accuracy: 0.9401\n",
      "Epoch 126/150\n",
      "250/250 [==============================] - 0s 659us/step - loss: 0.1382 - accuracy: 0.9399\n",
      "Epoch 127/150\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.1387 - accuracy: 0.9411\n",
      "Epoch 128/150\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.1390 - accuracy: 0.9401\n",
      "Epoch 129/150\n",
      "250/250 [==============================] - 0s 684us/step - loss: 0.1382 - accuracy: 0.9396\n",
      "Epoch 130/150\n",
      "250/250 [==============================] - 0s 673us/step - loss: 0.1387 - accuracy: 0.9410\n",
      "Epoch 131/150\n",
      "250/250 [==============================] - 0s 672us/step - loss: 0.1385 - accuracy: 0.9404\n",
      "Epoch 132/150\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.1382 - accuracy: 0.9396\n",
      "Epoch 133/150\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.1396 - accuracy: 0.9425\n",
      "Epoch 134/150\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.1379 - accuracy: 0.9414\n",
      "Epoch 135/150\n",
      "250/250 [==============================] - 0s 664us/step - loss: 0.1381 - accuracy: 0.9414\n",
      "Epoch 136/150\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.1382 - accuracy: 0.9415\n",
      "Epoch 137/150\n",
      "250/250 [==============================] - 0s 814us/step - loss: 0.1379 - accuracy: 0.9408\n",
      "Epoch 138/150\n",
      "250/250 [==============================] - 0s 820us/step - loss: 0.1378 - accuracy: 0.9417\n",
      "Epoch 139/150\n",
      "250/250 [==============================] - 0s 687us/step - loss: 0.1384 - accuracy: 0.9405\n",
      "Epoch 140/150\n",
      "250/250 [==============================] - 0s 675us/step - loss: 0.1381 - accuracy: 0.9408\n",
      "Epoch 141/150\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.1398 - accuracy: 0.9423\n",
      "Epoch 142/150\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.1386 - accuracy: 0.9414\n",
      "Epoch 143/150\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9396\n",
      "Epoch 144/150\n",
      "250/250 [==============================] - 0s 669us/step - loss: 0.1388 - accuracy: 0.9419\n",
      "Epoch 145/150\n",
      "250/250 [==============================] - 0s 742us/step - loss: 0.1386 - accuracy: 0.9417\n",
      "Epoch 146/150\n",
      "250/250 [==============================] - 0s 660us/step - loss: 0.1381 - accuracy: 0.9423\n",
      "Epoch 147/150\n",
      "250/250 [==============================] - 0s 671us/step - loss: 0.1383 - accuracy: 0.9396\n",
      "Epoch 148/150\n",
      "250/250 [==============================] - 0s 661us/step - loss: 0.1379 - accuracy: 0.9414\n",
      "Epoch 149/150\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.1381 - accuracy: 0.9394\n",
      "Epoch 150/150\n",
      "250/250 [==============================] - 0s 652us/step - loss: 0.1376 - accuracy: 0.9423\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(scaled_train, train_y, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "a8c9881c-9c1f-4e15-b540-36fdab12bbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = model.predict(scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "3eab4b29-d9b3-4c22-8c85-b2a00aba3742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.041898817\n",
      "RMSE:  0.204692\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"MSE: \", metrics.mean_squared_error(test_y, test_prediction))\n",
    "print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(test_y, test_prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165c882-85b3-4951-882a-04106467083b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ef17a-f848-4c96-b35f-2b649bc6be84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddbb02e-2659-4acd-85aa-bde96675953a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
